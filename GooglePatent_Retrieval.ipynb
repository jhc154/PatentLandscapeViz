{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pandas\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Google Patents Search https://patents.google.com/?q=crispr&country=US&status=GRANT&language=ENGLISH&type=PATENT\n",
    "#download results as gp-search-20200124-065711.csv\n",
    "#24 JAN 2020"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#start with a download of patent search with URLs to patent data\n",
    "df = pd.read_csv('gp-search-20200124-065711.csv', names = ['id', 'title', 'assignee', 'inventor/author', \n",
    "                                           'priority date', 'filing/creation date', \n",
    "                                           'publication date', 'grant date', 'result link', \n",
    "                                           'representative figure link'])[2:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#reset dataframe index\n",
    "df = df.reset_index(drop=True)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#create new columns to fill later with claim and classification data\n",
    "df['claim'] = \"\"\n",
    "df['classifications'] = \"\"\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#save URLs to patent data to a list, for retrieval\n",
    "urls = df['result link'].tolist()\n",
    "urls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#check to make sure that the list of URLs is == to the list of data, should match\n",
    "len(urls) == len(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "Using the URL list, scrape Google Patents for claims and classification data for each patent. \n",
    "\n",
    "In each iteration, append the data to a new column/row in the dataframe, df. \n",
    "\n",
    "May take hours to run. Please use the CSV file with raw text data if testing the scraper is not your objective. Located in repo. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "from selenium import webdriver\n",
    "from time import sleep\n",
    "\n",
    "driver = webdriver.Chrome('/Users/cheese/lData/PatentNLP/chromedriver')\n",
    "\n",
    "query_claims = '//*[@id=\"claims\"]'\n",
    "query_classes = '//*[@id=\"classifications\"]'\n",
    "\n",
    "df_claims = pd.DataFrame()\n",
    "\n",
    "# create an empty list \"claims\"\n",
    "claims = []\n",
    "# create an empty list \"classes\"\n",
    "classes = []\n",
    "# create an empty list \"classes\"\n",
    "anticipexpirations = []\n",
    "\n",
    "\n",
    "# create a for loop for computer to read through the list of urls \n",
    "for num, url in enumerate(urls, start=0):\n",
    "    # tell driver to get the url that the computer is reading\n",
    "    driver.get(url)\n",
    "    # tell chromedriver to find the claims in the pages that the driver is getting\n",
    "    \n",
    "    \n",
    "    print(\"url {}: {}\".format(num, url))\n",
    "    description_claim = driver.find_elements_by_xpath(query_claims)\n",
    "    # create a for loop (within the previous for loop) (*avoid nested for loops whenever possible!) \n",
    "    # to have computer look through each page for claims\n",
    "    for i in description_claim:\n",
    "        # if claims exist for the patent, add that to the empty list of claims\n",
    "        if i:\n",
    "            #create a list of claims based on each URL\n",
    "            #claims.append(i.text)\n",
    "            \n",
    "            #AND/OR write each claim to the data frame as you go\n",
    "            claim_text = i.text\n",
    "            df.at[num,'claim']=claim_text\n",
    "        # if no claims found, add \"none\"\n",
    "        else:\n",
    "            #write a value of 'NO-RECORD-RETRIEVED' if none is retrieved\n",
    "            #claims.append('NO-RECORD-RETRIEVED')\n",
    "            \n",
    "            #AND/OR write each claim to the data frame as you go\n",
    "            df.at[num,'claim']='NO-RECORD-RETRIEVED'\n",
    "    \n",
    "    # tell chromedriver to find the classes in the pages that the driver is getting\n",
    "    description_classes = driver.find_elements_by_xpath(query_classes)\n",
    "    # create a for loop (within the previous for loop) (*avoid nested for loops whenever possible!) \n",
    "    # to have computer look through each page for claims\n",
    "    for i in description_classes:\n",
    "        # if classes exist for the patent, add that to the empty list of classes\n",
    "        if i:\n",
    "            #classes.append(i.text)\n",
    "            classes_text = i.text\n",
    "            df.at[num, 'classifications']=classes_text\n",
    "        # if no classes found, add \"none\"\n",
    "        else:\n",
    "            #classes.append('None')\n",
    "            df.at[num, 'classifications']='NO-RECORD-RETRIEVED'\n",
    "            \n",
    "    # let the driver pause for 0.5 seconds between each page\n",
    "    sleep(0.25)\n",
    "# stop the driver\n",
    "print(\"done\")\n",
    "driver.quit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#check the table for results\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#save the scraped data to csv to avoid re-scraping\n",
    "df.to_csv('gp-search-20200124-065711-raw_with_claims.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
