{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A method for visualizing the pantent landscape.\n",
    "\n",
    "- Search Google Patents for a subject of interest.\n",
    "\n",
    "- Narrow the search for: \n",
    "\n",
    "- The target term or a combination of terms;\n",
    "\n",
    "- U.S. Patents;\n",
    "\n",
    "- In English Language;\n",
    "\n",
    "- Patents that have a status of Granted;\n",
    "\n",
    "- and any other paramters of interest.\n",
    "\n",
    "Objective: Vizually understand general themes and clusters of patented technology"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Google Patents Search https://patents.google.com/?q=crispr&country=US&status=GRANT&language=ENGLISH&type=PATENT\n",
    "#download results as gp-search-20200124-065711.csv\n",
    "#24 JAN 2020"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd #to use pandas\n",
    "\n",
    "#read in csv file containing raw text data retrieved from patent data source (Google)\n",
    "df = pd.read_csv('gp-search-20200124-065711-raw_with_claims.csv', index_col=0)\n",
    "\n",
    "import time #need to have date fields recognized as such\n",
    "\n",
    "# priority date\n",
    "df['priority date'] = pd.to_datetime(df['priority date'])\n",
    "# filing date\n",
    "df['filing/creation date'] = pd.to_datetime(df['filing/creation date'])\n",
    "# publication date\n",
    "df['publication date'] = pd.to_datetime(df['publication date'])\n",
    "# grant date. Some are not granted, so we tell pandas to ignore if they won't turn into datetime format\n",
    "df['grant date'] = pd.to_datetime(df['grant date'], errors = 'ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head() #verify the dataset - expecting it to be cleaned and processed "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create a list of words as stop_list to remove during text cleaning\n",
    "\n",
    "stop_list = ['abstract','abstracts','acceptable','apparatus','apparatuses',\\\n",
    "             'body','cancel','claim','claims','classification',\\\n",
    "             'classifications','comprise','comprises','comprising','composition'\\\n",
    "             'configure','configured','dependent','desire','description',\\\n",
    "             'device','devices','disclose','disclosed','discloses',\\\n",
    "             'embodiment','embodiments','example','examples',\\\n",
    "             'for example','herein','hide','includes','includes','invention',\\\n",
    "             'inventions','inventions','method','produce','present','provide',\\\n",
    "             'provided','provides','said','say','system','systems',\\\n",
    "             'thereof','user','subject','subsequent'] \n",
    "\n",
    "print(stop_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import libraries and prep to clean and tokenize raw text\n",
    "\n",
    "\n",
    "# install and import spacy (look up documentation for spacy)\n",
    "import spacy\n",
    "\n",
    "# import English package\n",
    "from spacy.lang.en import English\n",
    "\n",
    "# import string library\n",
    "import string\n",
    "\n",
    "# import regex to help clean text\n",
    "import re\n",
    "\n",
    "# import scikit learn package of English stop words \n",
    "from sklearn.feature_extraction.stop_words import ENGLISH_STOP_WORDS\n",
    "\n",
    "# assign variable \"punctuation\" to list of punctuations from string package\n",
    "punctuation = list(string.punctuation)\n",
    "\n",
    "# assign variable \"parser\" to the English function\n",
    "parser = English()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a function to clean and tokenize raw text\n",
    "def tokenizeText(text):\n",
    "    # clean text using regex\n",
    "    ## create a list of regex expressions and assign variable \"separators\"\n",
    "    separators = [\"\\xa0\\xa0\\xa0\\xa0\", \"\\r\", \"\\n\",\\\n",
    "                  \"\\t\", \"n't\", \"'m\", \"'ll\", '[^a-z ]'\\\n",
    "                 '[\\s]+',r'[^\\w]','^\\d+\\s|\\s\\d+\\s|\\s\\d+$']\n",
    "    \n",
    "    # iterate over the list of separators\n",
    "    for i in separators:\n",
    "        # every time regex finds a match in the text of the claims, delete (replace it with space)\n",
    "        text = re.sub(i, \" \", text.lower())\n",
    "    \n",
    "    # parse text using Spacy\n",
    "    tokens = parser(text)\n",
    "    tokens = [tok.lemma_.strip() for tok in tokens]\n",
    "    # get rid of words in the stop list\n",
    "    \n",
    "    return [tok for tok in tokens if len(tok) !=1 and tok not in stop]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create function to turn the list of tokens into one body of text (corpus)\n",
    "def text_processing(corp):\n",
    "    # call the tokenizeText function we created above\n",
    "    corp = tokenizeText(corp)\n",
    "    return ' '.join(corp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop = set(list(stop_list) + list(ENGLISH_STOP_WORDS) + list(punctuation))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# apply the function for each row of texts in the text column of the dataframe\n",
    "df['claim'] = df['claim'].apply(text_processing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['classifications'] = df['classifications'].apply(text_processing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# take a look at what the function returned\n",
    "print(df['claim'][5:])\n",
    "print('')\n",
    "print(df['classifications'][5:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#clean up white space, just making sure\n",
    "df['claim'] = df['claim'].str.strip()\n",
    "df['classifications'] = df['classifications'].str.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#reset index\n",
    "df = df.reset_index(drop=True)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import pandas as pd\n",
    "                  \n",
    "tfidf_vectorizer = TfidfVectorizer(\n",
    "    min_df=0.1,\n",
    "    max_df=0.9,\n",
    "    max_features = 5000, \n",
    "    stop_words = 'english')\n",
    "    \n",
    "tfidf_matrix = tfidf_vectorizer.fit_transform(df.claim)\n",
    "type(tfidf_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#https://medium.com/@dmitriy.kavyazin/principal-component-analysis-and-k-means-clustering-to-visualize-a-high-dimensional-dataset-577b2a7a5fe2\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "pca = PCA(n_components=20)\n",
    "principalComponents = pca.fit_transform(tfidf_matrix.todense())\n",
    "PCA_components = pd.DataFrame(principalComponents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "features = range(pca.n_components_)\n",
    "plt.bar(features, pca.explained_variance_ratio_, color='blue')\n",
    "plt.xlabel('PCA features')\n",
    "plt.ylabel('variance %')\n",
    "plt.xticks(features);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#run elbow analysis\n",
    "from sklearn.cluster import MiniBatchKMeans\n",
    "\n",
    "ks = range(1, 10)\n",
    "inertias = []\n",
    "for k in ks:\n",
    "    # Create a KMeans instance with k clusters: model\n",
    "    model = MiniBatchKMeans(n_clusters=k)\n",
    "    \n",
    "    # Fit model to samples\n",
    "    model.fit(PCA_components.iloc[:,:3])\n",
    "    \n",
    "    # Append the inertia to the list of inertias\n",
    "    inertias.append(model.inertia_)\n",
    "    \n",
    "plt.plot(ks, inertias, '-o', color='black')\n",
    "plt.xlabel('number of clusters, k')\n",
    "plt.ylabel('inertia')\n",
    "plt.xticks(ks)\n",
    "plt.title('Elbow Curve - CRISPR Patent Research')\n",
    "\n",
    "plt.savefig('PatentViz-CRISPR-Elbow.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#use elbow analysis to determine what to set n_clusters to\n",
    "import numpy as np\n",
    "\n",
    "clusters = MiniBatchKMeans(n_clusters=4, init_size=1024, batch_size=2048, random_state=20).fit_predict(tfidf_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_top_keywords(data, clusters, labels, n_terms):\n",
    "    df_gtk = pd.DataFrame(data.todense()).groupby(clusters).mean()\n",
    "    \n",
    "    #print(df_gtk)\n",
    "    for i,r in df_gtk.iterrows():\n",
    "        print('\\nCluster {}'.format(i))\n",
    "        print(','.join([labels[t] for t in np.argsort(r)[-n_terms:]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#return clusters and top words\n",
    "get_top_keywords(tfidf_matrix, clusters, tfidf_vectorizer.get_feature_names(), 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#calculate centers\n",
    "centers = np.array(model.cluster_centers_)\n",
    "#https://matplotlib.org/gallery/shapes_and_collections/scatter.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#3d plot clusters \n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "\n",
    "fig = px.scatter_3d(x=PCA_components[0], y=PCA_components[1], z=PCA_components[2],\n",
    "                    color=clusters, hover_name=clusters, \n",
    "                    color_continuous_scale=[\"tomato\", \"forestgreen\", \"cornflowerblue\", \"gold\"]\n",
    "                    )\n",
    "\n",
    "fig.update_layout(\n",
    "    height=600, \n",
    "    title_text='PCA Cluster Plot - CRISPR Patent Search 3D')\n",
    "    \n",
    "    \n",
    "fig.add_trace(go.Scatter3d(\n",
    "    x=[centers[0][0], centers[1][0], centers[2][0], centers[3][0], centers[4][0]],\n",
    "    y=[centers[0][1], centers[1][1], centers[2][1], centers[3][1], centers[4][1]],\n",
    "    z=[centers[0][2], centers[1][2], centers[2][2], centers[3][2], centers[4][2]],\n",
    "    name = 'Cluster Center',\n",
    "    marker_color=\"black\", \n",
    "    marker_size=16,\n",
    "    mode='lines+text',\n",
    "    text= [' ',' ',' ',' ',' '], \n",
    "    textposition = \"top center\"\n",
    "    \n",
    "))\n",
    "\n",
    "fig.update_traces(textfont_size=12)\n",
    "\n",
    "\n",
    "#update legend bar title\n",
    "fig.update_layout(coloraxis_colorbar=dict(\n",
    "    title=\"Cluster\"))\n",
    "\n",
    "\n",
    "fig.update_layout(\n",
    "    scene=go.layout.Scene(\n",
    "        aspectratio=dict(\n",
    "            x=1,\n",
    "            y=1,\n",
    "            z=1\n",
    "        ),\n",
    "\n",
    "\n",
    "        annotations=[dict(\n",
    "            x=centers[0][0],\n",
    "            y=centers[0][1],\n",
    "            z=centers[0][2],\n",
    "            text=\"progeny,seed,plant,soybean\",\n",
    "            textangle=0,\n",
    "            ax=0,\n",
    "            ay=-75,\n",
    "            font=dict(\n",
    "                color=\"black\",\n",
    "                size=12\n",
    "            ),\n",
    "            arrowcolor=\"black\",\n",
    "            arrowsize=3,\n",
    "            arrowwidth=1,\n",
    "            arrowhead=1\n",
    "        ), dict(\n",
    "            x=centers[1][0],\n",
    "            y=centers[1][1],\n",
    "            z=centers[1][2],\n",
    "            text=\"acid,sequence,seq,pron\",\n",
    "            textangle=0,\n",
    "            ax=-50,\n",
    "            ay=-75,\n",
    "            font=dict(\n",
    "                color=\"black\",\n",
    "                size=12\n",
    "            ),\n",
    "            arrowcolor=\"black\",\n",
    "            arrowsize=3,\n",
    "            arrowwidth=1,\n",
    "            arrowhead=1\n",
    "        ), dict(\n",
    "            x=centers[2][0],\n",
    "            y=centers[2][1],\n",
    "            z=centers[2][2],\n",
    "            ax=20,\n",
    "            ay=-15,\n",
    "            font=dict(\n",
    "                color=\"black\",\n",
    "                size=12\n",
    "            ),\n",
    "            text=\"maize,variety,seed,plant\",\n",
    "            arrowhead=1,\n",
    "            xanchor=\"left\",\n",
    "            yanchor=\"bottom\"\n",
    "        ), dict(\n",
    "            x=centers[3][0],\n",
    "            y=centers[3][1],\n",
    "            z=centers[3][2],\n",
    "            text=\"gene,target,sequence,cell\",\n",
    "            textangle=0,\n",
    "            ax=45,\n",
    "            ay=-75,\n",
    "            font=dict(\n",
    "                color=\"black\",\n",
    "                size=12\n",
    "            ),\n",
    "            arrowcolor=\"black\",\n",
    "            arrowsize=3,\n",
    "            arrowwidth=1,\n",
    "            arrowhead=1\n",
    "        )\n",
    "                    ]\n",
    "    ),\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "fig.update_layout(scene = dict(\n",
    "                    xaxis_title='PCA 1',\n",
    "                    yaxis_title='PCA 2',\n",
    "                    zaxis_title='PCA 3'))\n",
    "\n",
    "fig.show();\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Alternative ways to visualize the same data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plt.scatter(PCA_components[0], PCA_components[1], alpha=.1, color='black')\n",
    "plt.xlabel('PCA 1')\n",
    "plt.ylabel('PCA 2')\n",
    "plt.title('PCA Cluster Plot - CRISPR Patent Search B/W')\n",
    "plt.savefig('PatentViz-CRISPR-bw');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#3d plot clusters \n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "\n",
    "fig = px.scatter(x=PCA_components[0], y=PCA_components[1], color=clusters, hover_name=clusters,\n",
    "                color_continuous_scale=[\"tomato\", \"forestgreen\", \"cornflowerblue\", \"gold\"])\n",
    "\n",
    "fig.update_layout(\n",
    "    height=600, \n",
    "    title_text='PCA Cluster Plot - CRISPR Patent Search 2D')\n",
    "\n",
    "#update legend bar title\n",
    "fig.update_layout(coloraxis_colorbar=dict(\n",
    "    title=\"Cluster\"))\n",
    "\n",
    "fig.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next Project - Deploying graphs to production with Dash."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "References: \n",
    "\n",
    "https://plot.ly/python/\n",
    "\n",
    "https://medium.com/plotly/introducing-plotly-express-808df010143d\n",
    "\n",
    "https://plot.ly/python/text-and-annotations/\n",
    "\n",
    "https://medium.com/@dmitriy.kavyazin/principal-component-analysis-and-k-means-clustering-to-visualize-a-high-dimensional-dataset-577b2a7a5fe2\n",
    "\n",
    "https://jakevdp.github.io/PythonDataScienceHandbook/05.09-principal-component-analysis.html\n",
    "\n",
    "https://plot.ly/python/v3/ipython-notebooks/color-scales/\n",
    "\n",
    "https://community.plot.ly/t/plotly-colours-list/11730/3\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
